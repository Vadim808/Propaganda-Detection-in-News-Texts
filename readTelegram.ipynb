{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4md6QGwOvUZb",
    "outputId": "137982f7-4de0-478b-aa15-43e724100e3d"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZilYxh6Ir2As",
    "outputId": "6be9f368-216d-4c72-939a-c9d3057bb1e9"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aosqMa1OrmBf"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    " \n",
    "with open(\"/content/drive/MyDrive/Diplom/ChatExport_2021-05-02/messages.html\", \"r\") as f:\n",
    "    contents = f.read()\n",
    "    soup = BeautifulSoup(contents, 'lxml')\n",
    "    texts = soup.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yhu57NearmBi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "result_text = []\n",
    "text_list = texts.split(\"\\n\")\n",
    "mes = \"Not included, change data exporting settings to download.\"\n",
    "for i in range(len(text_list)):\n",
    "    text = text_list[i]\n",
    "    if len(text) > 100 and text != mes:\n",
    "        ind = text.find(\"http\")\n",
    "        text = text[:ind]\n",
    "        result_text += [text]\n",
    "result_text = np.array(result_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5W0OVo3YrmBj"
   },
   "outputs": [],
   "source": [
    "labels = [\n",
    "\"Reductio ad hitlerum\",\n",
    "\"Whataboutism\",\n",
    "\"Presenting Irrelevant Data (Red Herring)\",\n",
    "\"Doubt\",\n",
    "\"Slogans\",\n",
    "\"Appeal to fear/prejudice\",\n",
    "\"Obfuscation, Intentional vagueness, Confusion\",\n",
    "\"Misrepresentation of Someone's Position (Straw Man)\",\n",
    "\"Glittering generalities (Virtue)\",\n",
    "\"Appeal to authority\",\n",
    "\"Repetition\",\n",
    "\"Bandwagon\",\n",
    "\"Causal Oversimplification\",\n",
    "\"Name calling/Labeling\",\n",
    "\"Thought-terminating clich√©\",\n",
    "\"Flag-waving\",\n",
    "\"Exaggeration/Minimisation\",\n",
    "\"Smears\",\n",
    "\"Loaded Language\",\n",
    "\"Black-and-white Fallacy/Dictatorship\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xyk4rxCtrmBk"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "trvLzpLqrmBk"
   },
   "outputs": [],
   "source": [
    "def preprocessing(texts, max_len=300):\n",
    "    col_input_ids = []\n",
    "    col_attention_mask = []\n",
    "    col_token_type_ids = []\n",
    "    technique = []\n",
    "    res_texts = []\n",
    "    \n",
    "    for i in range(texts.shape[0]):\n",
    "        token_text = tokenizer.encode_plus(\n",
    "            texts[i], \n",
    "            return_offsets_mapping=True,\n",
    "            max_length=max_len,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        token_count = len(token_text.input_ids)\n",
    "        \n",
    "        for label in labels:\n",
    "            token_technique = tokenizer.encode_plus(\n",
    "                label, \n",
    "                return_offsets_mapping=True, \n",
    "                max_length=max_len, \n",
    "                truncation=True\n",
    "            )\n",
    "            input_ids = token_text.input_ids + token_technique.input_ids[1:]\n",
    "            token_type_ids = [0] * token_count + [1] * len(token_technique.input_ids[1:])\n",
    "            len_input_ids = len(input_ids)\n",
    "            attention_mask = [1] * len_input_ids\n",
    "            \n",
    "            if max_len < len_input_ids:\n",
    "                break\n",
    "\n",
    "            technique.append(label)\n",
    "            res_texts.append(texts[i])\n",
    "            padding = [0] * (max_len - len_input_ids)\n",
    "            col_input_ids.append(input_ids + padding)\n",
    "            col_attention_mask.append(attention_mask + padding)\n",
    "            col_token_type_ids.append(token_type_ids + padding)\n",
    "\n",
    "    return col_input_ids, col_attention_mask, col_token_type_ids, technique, res_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "691mnp2UrmBl"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "col_input_ids, col_attention_mask, col_token_type_ids, technique, result_text = preprocessing(result_text, max_len=300)\n",
    "data_pd = pd.DataFrame()\n",
    "data_pd[\"text\"] = result_text\n",
    "data_pd[\"technique\"] = technique\n",
    "data_pd[\"col_input_ids\"] = col_input_ids\n",
    "data_pd[\"col_attention_mask\"] = col_attention_mask\n",
    "data_pd[\"col_token_type_ids\"] = col_token_type_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aYWQb1R7rmBl"
   },
   "outputs": [],
   "source": [
    "def check_text(model, data, i, dev, line):\n",
    "    ids = torch.tensor([list(data[\"col_input_ids\"][i])]).to(dev)\n",
    "    attention_mask = torch.tensor([list(data[\"col_attention_mask\"][i])]).to(dev)\n",
    "    type_ids = torch.tensor([list(data[\"col_token_type_ids\"][i])]).to(dev)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(ids, attention_mask, type_ids)\n",
    "        ans_mask = (torch.squeeze(output, dim=1)[0] > line).cpu()\n",
    "        if sum(ans_mask) > 0:\n",
    "            print(\"technique:\", data[\"technique\"][i])\n",
    "            print(\"---\")\n",
    "            print(\"text:\", data[\"text\"][i])\n",
    "            print(\"---\")\n",
    "            ans_seq_tok = np.array(data[\"col_input_ids\"][i])[ans_mask == 1]\n",
    "            print(\"ans:\", tokenizer.decode(ans_seq_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z6GypwDwxJld"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import transformers\n",
    "\n",
    "class Model(transformers.BertPreTrainedModel):\n",
    "    def __init__(self, config, PATH):\n",
    "        super(Model, self).__init__(config)\n",
    "        self.bert = transformers.BertModel.from_pretrained(PATH)\n",
    "        self.linear = nn.Linear(768, 1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.sigm = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        embedding = self.bert(\n",
    "            ids,\n",
    "            attention_mask=mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )[0]\n",
    "        logits = self.linear(embedding)\n",
    "        logits = self.flatten(logits)\n",
    "        result = self.sigm(logits)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V8Kb7Eu3xpvN"
   },
   "outputs": [],
   "source": [
    "PATH = \"DeepPavlov/rubert-base-cased\"\n",
    "\n",
    "bert = transformers.BertModel.from_pretrained(PATH)\n",
    "model = Model(bert.config, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6vw5RRPdrmBm",
    "outputId": "b5bf3971-8b61-43b7-a9a2-8b9a299ba33b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "PATH = \"/content/drive/MyDrive/Diplom/model2.pth\"\n",
    "model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "b98X3eJXwbid",
    "outputId": "fe7d192d-bbd7-446e-d552-cea4d12b8c7f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "else:  \n",
    "  dev = \"cpu\" \n",
    "\n",
    "dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iXp7JoxdyAYp",
    "outputId": "5089a1e3-1052-4c88-ec44-94082cb5e4f5"
   },
   "outputs": [],
   "source": [
    "model.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "csxsjKEPyVXX",
    "outputId": "bfceaa87-ee2e-4929-a982-103ce770943e"
   },
   "outputs": [],
   "source": [
    "data_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ao6CFEmgrmBm",
    "outputId": "cdf6fdca-cac2-442e-b07c-242ea279aaca"
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    check_text(model, data_pd, i, dev, 0.5)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "readTelegram.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
